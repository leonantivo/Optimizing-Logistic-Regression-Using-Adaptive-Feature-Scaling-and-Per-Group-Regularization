# -*- coding: utf-8 -*-
"""Adaptive-Scaling-and-Group-Specific-Regularization-with-All-Plots.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d9H5pW2sA4J7G3pE6Q6zV5v4W1B8u0gT
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve
from sklearn.metrics import precision_score, recall_score
from IPython.display import display

# Load dataset
data = load_breast_cancer()
X, y = data.data, data.target
features = data.feature_names

# --- Adaptive Scaling ---

# Define feature groups
G1_idx = list(range(0, 10))
G2_idx = list(range(10, 20))
G3_idx = list(range(20, 30))

# --- Group-Specific Regularization and 5-Fold Cross-Validation ---

# Initialize lists to store metrics and plotting data for each fold
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
roc_auc_scores = []
all_y_test = []
all_final_proba = []

# Initialize K-Fold Cross-Validation
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Loop through each fold
for fold, (train_index, test_index) in enumerate(kf.split(X)):
    print(f"--- Fold {fold+1}/{n_splits} ---")

    # Split data for the current fold
    X_train_fold, X_test_fold = X[train_index], X[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # --- Adaptive Scaling within the loop to avoid data leakage ---
    X_train_G1, X_train_G2, X_train_G3 = X_train_fold[:, G1_idx], X_train_fold[:, G2_idx], X_train_fold[:, G3_idx]
    X_test_G1, X_test_G2, X_test_G3 = X_test_fold[:, G1_idx], X_test_fold[:, G2_idx], X_test_fold[:, G3_idx]

    # Initialize and fit scalers ONLY on the training data for this fold
    scaler_G1 = StandardScaler().fit(X_train_G1)
    scaler_G2 = MinMaxScaler().fit(X_train_G2)
    scaler_G3 = RobustScaler().fit(X_train_G3)

    # Transform both training and testing data using the fitted scalers
    X_train_G1_scaled = scaler_G1.transform(X_train_G1)
    X_train_G2_scaled = scaler_G2.transform(X_train_G2)
    X_train_G3_scaled = scaler_G3.transform(X_train_G3)

    X_test_G1_scaled = scaler_G1.transform(X_test_G1)
    X_test_G2_scaled = scaler_G2.transform(X_test_G2)
    X_test_G3_scaled = scaler_G3.transform(X_test_G3)

    # Concatenate the scaled groups back into a single feature matrix
    X_train_scaled = np.concatenate([X_train_G1_scaled, X_train_G2_scaled, X_train_G3_scaled], axis=1)
    X_test_scaled = np.concatenate([X_test_G1_scaled, X_test_G2_scaled, X_test_G3_scaled], axis=1)

    # --- Group-Specific Regularization ---
    X_train_G1G2 = X_train_scaled[:, :20]
    X_test_G1G2 = X_test_scaled[:, :20]
    X_train_G3 = X_train_scaled[:, 20:]
    X_test_G3 = X_test_scaled[:, 20:]

    # Train a Logistic Regression model for Group 1 and 2 (L2 penalty)
    model_G1G2 = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)
    model_G1G2.fit(X_train_G1G2, y_train_fold)

    # Train a separate Logistic Regression model for Group 3 (L1 penalty)
    model_G3 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)
    model_G3.fit(X_train_G3, y_train_fold)

    # --- Combine Predictions and Evaluate for this fold ---
    proba_G1G2 = model_G1G2.predict_proba(X_test_G1G2)[:, 1]
    proba_G3 = model_G3.predict_proba(X_test_G3)[:, 1]
    final_proba = (proba_G1G2 + proba_G3) / 2
    final_pred = (final_proba > 0.5).astype(int)

    # Calculate and store performance metrics for this fold
    accuracy_scores.append(accuracy_score(y_test_fold, final_pred))
    precision_scores.append(precision_score(y_test_fold, final_pred))
    recall_scores.append(recall_score(y_test_fold, final_pred))
    f1_scores.append(f1_score(y_test_fold, final_pred))
    roc_auc_scores.append(roc_auc_score(y_test_fold, final_proba))

    # Store data for visualization
    all_y_test.extend(y_test_fold)
    all_final_proba.extend(final_proba)

# --- Calculate and display average results across all folds ---
avg_accuracy = np.mean(accuracy_scores)
avg_precision = np.mean(precision_scores)
avg_recall = np.mean(recall_scores)
avg_f1 = np.mean(f1_scores)
avg_roc_auc = np.mean(roc_auc_scores)

print("\n\n--- Average Results Across 5 Folds ---")
final_results = {
    "Accuracy": avg_accuracy,
    "Precision": avg_precision,
    "Recall": avg_recall,
    "F1 Score": avg_f1,
    "ROC AUC": avg_roc_auc
}
display(pd.DataFrame([final_results]))

# --- Visualization using Matplotlib and Seaborn ---

# 1. Plot all metrics in a bar chart
plt.figure(figsize=(10, 6))
bars = plt.bar(final_results.keys(), final_results.values(), color='cornflowerblue')
plt.ylim(0, 1)
plt.title("Average Performance Metrics of AFS + PGR Model (5-Fold CV)\n")
plt.ylabel("Score")
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f'{yval:.2f}', ha='center', va='bottom')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# 2. ROC Curve (using all data from the folds)
# Concatenate all true labels and predicted probabilities from all folds
fpr, tpr, _ = roc_curve(all_y_test, all_final_proba)
roc_auc = roc_auc_score(all_y_test, all_final_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Aggregated)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print("\n")

# 3. Confusion Matrix (using predictions from the final fold)
# Re-run prediction on the last fold's data to get the final predictions
final_pred_last_fold = (final_proba > 0.5).astype(int)
cm = confusion_matrix(y_test_fold, final_pred_last_fold)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title(f'Confusion Matrix (Fold {n_splits})')
plt.show()
print("\n")

# 4. Feature Importance (Logistic Regression Coefficients from the final fold)
coef_G1G2 = model_G1G2.coef_[0]
coef_G3 = model_G3.coef_[0]
combined_coef = np.concatenate([coef_G1G2, coef_G3])
combined_features = list(features[:20]) + list(features[20:])

plt.figure(figsize=(15, 8))
plt.barh(combined_features, combined_coef, color='skyblue')
plt.xlabel('Coefficient Magnitude')
plt.ylabel('Feature Name')
plt.title('Feature Importance (Logistic Regression Coefficients from Fold 5)')
plt.grid(axis='x', linestyle='--')
plt.tight_layout()
plt.show()
print("\n")

# 5. Precision-Recall Curve (using all data from the folds)
precision, recall, _ = precision_recall_curve(all_y_test, all_final_proba)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='purple', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall (True Positive Rate)')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve (Aggregated)')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid(True)
plt.legend(loc="lower left")
plt.show()
